---
title: "Tarea 2 Propiedades de los modelos ARMA"
author: "Cuéllar, Eduardo, García Jesús, Miranda Areli, Ramirez José, Saldaña Ricardo "
date: "10/26/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Ejercicio 1  
1.- Considere el proceso $MA(2)$:

$$ X_t = Z_t - 0.4Z_{t-1} - 1.2Z_{t-2} $$
donde ${Z_{t}}$ es un ruido blanco Gaussiano.

(a) Calcule $\sigma^{2}_{X}$ suponiendo que $\sigma^{2}_{Z}$ = 1.
(b) Encuentre la expresión general para la función de autocorrelación $\rho_{k}$.
(c) Grafique $\rho_{k}$ (correlograma ACF), para $k= 0,1,2,...,10$.
(d) Encuentre la expresión general para la funció de autocorrelación parcial $\phi_{kk}$.
(e) Grafique $\phi_{kk}$ (correlograma PACF), para $k = 0,1,2,...,10$.
(f) En R simule el proceso ${X_{t}}$ para un tamaño de muestra $n$, grafique la serie de tiempo y los correlogramas ACF y PACF.
Compare los correlogramas simulados con los del proceso original.

```{r,message=FALSE}
#Cargamos librerías
library(ggplot2);library(itsmr);library(forecast);library(TSA);library(lmtest)
library(timeSeries);library(timeSeries);library(astsa);
library(tseries);library(forecast);library(nortest)
```

Respuesta: 

a) $Var$($X_{t}$) = $Var(Z_{t} - 0.4_{t-1}, - 1.2Z_{t-2})$ $\dots$ (1)

Como $Z_{k}$ $\perp$ $Z_{j}$ $\forall$ $k \ne j$, podemos expresar a (1) de la siguiente manera:

 
\begin{align*} &= Var(Z_{t}) + Var(-0.4 Z_{t-1}) + Var(-1.2Z_{t-2})\\ 
               &=  Var(Z_{t}) + (-0.4)^{2} Var(Z_{t-1}) + (-1.2)^{2} Var(Z_{t-2}) \dots (2)
\end{align*}

Como $Z_{t}$ son $v.a.i.i.d.$, con $\mathbb{E}[Z_{t}] = 0$ y $Var(Z_{t}) = 1$

 
\begin{align*}(2) &= 1 + (.16)(1) + (1.44)(1)\\
                  &= 1 + .16 + 1.44\\ 
                  &= 2.6  Cov(Z_{t} - 1.4Z_{t-1} - 1.2Z_{t-2}, Z_{t+k} - 0.4Z_{t-1+k} - Z_{t-2+k}) 
\end{align*} $_\blacksquare$

    
    
b) Veamos la autocovarianza:

 
\begin{align*} \gamma(k) &= Cov(X_{t}, X_{t+k})\\ 
                         &= Cov(Z_{t} - 0.4Z_{t-1} - 1.2Z_{t-2}, Z_{t+k} - 0.4Z_{t-1+k} - 1.2Z_{t-2+k})\\
                         &= Cov(Z_{t}, Z{_t+k}) - 0.4 Cov(Z_{t}, Z_{t+k-1}) - 1.2 Cov(Z_{t}, Z_{t+k-2})\\
                         &-0.4 Cov(Z_{t-1},Z_{t+k}) + .16Cov(Z_{t-1},Z_{t+k-1}) + .48Cov(Z_{t-1}, Z_{t+k-2})\\
                         &-1.2Cov(Z_{t-2}, Z_{t+k}) + .48 Cov(Z_{t-2}, Z_{t+k_1}) + 1.44 Cov(Z_{t-2}, Z_{t+k-2})
\end{align*}

    
  
Si $k = 0$, entonces:


\begin{align*} \gamma(0) &= Cov(Z_{t}, Z_{t}) + .16 Cov(Z_{t-1}, Z_{t+k-1}) + 1.44 Cov(Z_{t-2}, Z_{t+k-2})\\
                         &= Var(Z_{t}) + .16Var(Z_{t}) + 1.44 Var(Z_{t})\\ 
                         &= 2.6\\
                         &= Var(X_{t}).
\end{align*}

    
Y 0 en los demás, ya que $Z_{j}$ $\forall$ $k \ne j$

Si $k = 1$ 

 
\begin{align*} \gamma(1) &= -0.4Cov(Z_{t}, Z_{t+1-1}) + .48 Cov(Z_{t-1}, Z_{t+1-2})\\
                         &= -0.4Cov(Z_{t}, Z_{t}) + .48 Cov(Z_{t-1}, Z_{t-1})\\ 
                         &= -0.4Var(Z_{t}) + .48 Var(Z_{t})\\
                         &= -0.4 + .48\\
                         &= .08
\end{align*}

Si $k = -1$ 

 
\begin{align*} \gamma(-1) &= -0.4Cov(Z_{t-1}, Z_{t-1}) + .48 Cov(Z_{t-2}, Z_{t-1-1})\\
                          &= -0.4Cov(Z_{t-1}, Z_{t-1}) + .48 Cov(Z_{t-2}, Z_{t-2})\\ 
                          &= -0.4Var(Z_{t}) + .48 Var(Z_{t})\\
                          &= -0.4 + .48\\
                          &= .08
\end{align*}

Si $k = 2$ 

 
\begin{align*} \gamma(2) &= -1.2Cov(Z_{t}, Z_{t+2-2})\\
                         &= -1.2Cov(Z_{t}, Z_{t})\\ 
                         &= -1.2Var(Z_{t})\\
                         &= -1.2\\
\end{align*}


Si $k = -2$ 

 
\begin{align*} \gamma(-2) &= -1.2Cov(Z_{t-2}, Z_{t-2})\\
                          &= -1.2Cov(Z_{t-2}, Z_{t-2})\\ 
                          &= -1.2Var(Z_{t})\\
                          &= -1.2\\
\end{align*}


Son los únicos $k$ para los cuales hay un término tal que, en $Cov(Z_{j}, Z_{i})$ tenemos $i = j$, y como $Z_{j} \perp Z_{i}$ $\forall j \ne i$ $\implies  Cov(Z_{j}, Z_{i}) = 0$

$$\begin{aligned} \therefore \gamma(k) = \begin{cases} 
                             2.6   & k = 6 \\
                             0.08 & |k| = 1 \\
                            -1.2 &  |k|=2\\
                             0 & e.o.c
                            \end{cases}
\end{aligned}$$

Entonces, tenemos que:

Para $k = 1$:

\begin{align*} \rho_{k} = 1 
\end{align*}

Para $|k| = 1$:

\begin{align*} \rho_{k} = \frac{0.08}{2.6}
                        &= \frac{2}{65}
\end{align*}

Para $|k| = 2$:

\begin{align*} \rho_{k} = \frac{-1.2}{2.6}
                        &= \frac{-6}{13}
\end{align*}

Para $|k| > 2$:

\begin{align*} \rho_{k} = 0 
\end{align*} $_\blacksquare$


c) Gráfica:
```{r}
acf_coefs_ej1=c(1,2/65,-6/13)
#llenemos de 0 los faltantes
for (i in (length(acf_coefs_ej1)+1):11){
  acf_coefs_ej1[i]=0
}
ACF_1<-data.frame('ACF'=acf_coefs_ej1,lag=0:10)
ACF_1
ggplot(ACF_1,aes(x=lag,y=ACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```


d) Automatizaremos la obtención de los coeficientes del PACF para el ejercicio 1:
```{r}
#x será el vector con los coeficientes de autocorrelación
coefs_pacf<-function(p,k){
  if(k==0){
    return(1)
  }
  if(length(p)<k+1){
    for(i in length(p):k){
      p[i+1]=0
    }
  }
  A<-matrix(nrow=k,ncol = k)
  for (j in 1:k){
    for (i in 1:k){
       A[i,j]=p[abs(i-j)+1]
    }
  }
  B<-A
  for (i in 1:k){
    B[i,k]=p[i+1]
  }
  return(det(B)/det(A))
}
```
¿Cómo funciona? Bien, en clase, en la página 27 de las notas, podemos observar
que $\phi_{kk}$, que es el coeficiente de autocorrelación parcial para un lag de $k$
se puede calcular usando Cramer. Observamos el patrón de que en la matriz que 
'va en el denominador', iba el coeficiente $\rho_{i}$ donde $i$ era el valor absoluto
de la diferencia entre el número de columna y renglón, por ello es que llenamos la
matriz como $A[i,j]=p[abs(i-j)+1]$. En la matriz 'numerador', únicamente es cambiar
el último renglón por los $\rho_j$ donde $j$ es el número de renglón, siendo ambas
matrices de dimensión $k*k$

Ahora solo aplicamos la fórmula:
```{r}
pacf_ej1=c()
for (i in 1:11){
  pacf_ej1[i]<-coefs_pacf(acf_coefs_ej1,i-1)
}
PACF_1=data.frame(PACF=pacf_ej1,lag=c(0:10))
PACF_1
```

e) Graficamos

```{r}
ggplot(PACF_1,aes(x=lag,y=PACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```

f) Simulamos 

```{r}
par(mfrow=c(1,1))
MA2=arima.sim(list(order=c(0,0,2), ma=c(-0.4,-1.2)),n=500)
plot(MA2, main="Simulación MA(2)", col="purple", ylab="", las=1, xlab="Tiempo")

# Vamos a calcular el ACF de la muestra simulada
(ACFMA=ARMAacf( ma=c(-0.4,-1.2), ar=0,15))
plot(ACFMA[-1],type="h", main="ACF de la muestra de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=2, col="violet")
acf(MA2)

# Vamos a calcular el PACF de la muestra simulada 
(PACFMA=ARMAacf( ma=c(-0.4,-1.2), ar=0,15, pacf=T))
plot(PACFMA,type="h", main="PACF de la muestra de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=1, col="violet")
pacf(MA2)
```

## Ejercicio 2

2.- Considere el proceso $AR(2)$:

$$ X_{t} + 0.4X_{t-1} + 0.75X_{t-2} = Z_{t} $$

donde {$Z_{t}$} es un ruido blanco Gaussiano.

(a) Calcule $\sigma^{2}_{X}$ suponiendo que $\sigma{^{2}_{Z}} = 1$.
(b) Encuentre la expresión general para la función de autocorrelación $\rho_{k}$.
(c) Grafique $\rho_{k}$ (correlograma ACF), para $k = 0,1,2, \dots, 10$.
(d) Encuentre la expresión general para la función de autocorrelación parcial  $\phi_{kk}$.
(e) Grafique $\phi_{kk}$ (correlograma PACF), para $k = 0,1,2, \dots, 10$.
(f) En R simule el proceso ${X_{t}}$ para un tamaño de muestra $n$ grafique la serie de tiempo y los correlogramas ACP y PACF.
Compare los correlogramas simulados con los del proceso original.

Primero, notemos que el proceso AR(2) lo podemos ver de la siguiente materia:
$$ X_{t} = -.04 X_{t-1} - 0.75 X_{t-2} + Z{t} $$
Además, notemos que: $\phi_{1} = -0.4$ y $\phi_{2} = - 0.75.$ 

Igualmente, las raíces del polinomio de retraso caen fuera del círculo complejo
unitario:
```{r}
x<-c(1,0.4,0.75)
raices<-polyroot(x)
#Las raíces son:
raices

#Ahora veamos que la norma es:
norm(cbind(-0.266667,1.123487))
norm(cbind(-0.266667,-1.123487))
```


Una vez lo anterior, pasemos al inciso a).

a) En clase vimos lo siguiente:

$$ Var(X_{t}) = \gamma(0) = \frac{\sigma^{2}_{Z}}{1 - \phi_{1}\rho(1) - \phi_{2}\rho(2) - \dots - \phi_{p}\rho(p) } $$
De esta manera, sabemos que debemos calcular $\rho(1)$ y $\rho(2)$. De igual manera, sabemos que, por lo visto en clase, que para un $AR(p)$ lo calculamos de la siguiente manera:

$$\rho(k) = \phi_{1}\rho(k - 1) + \phi_{2}\rho(k - 2)+ \dots + \phi_{p}\rho(k - p)  $$
Con $\rho(0) = 1$ y $\rho(-k) = \rho(p)$.

Así, para $k = 1$ y $k = 2$ tenemos:

\begin{align*}
\rho(1) &= -0.4 \rho(0) - 0.75 \rho(1) \implies\\ 
\rho(2) &= -0.4 \rho(1) - 0.75 \rho(0) 
\end{align*}

Entonces:

\begin{align*} 
\rho(1) &= -0.4 - 0.75 \rho(1) \implies  1.75 \rho(1) = -0.4\\
\implies \rho(1) &= \frac{-0.4}{1.75}\\ 
        &= \frac{-8}{35} 
\end{align*}

\begin{align*} 
\rho(2) &= -0.4\rho(1) - 0.75\\ 
\implies \rho(2) &= -0.4 \left(\frac{-0.4}{1.75}\right)- .75\\ 
        &= \frac{-461}{700} 
\end{align*} $\blacksquare$

Nota: Como necesitamos los coeficientes de autocorrelación, el ejercicio continua despúes del inciso c), a partir de donde dice "Entonces".


b) Programamos la función recursiva para los coeficientes de correlación
```{r}
#phi es un vector con los coeficientes del modelo AR(p)
#p es un vector con los acf para lag=1 y 2
coefs_acf<-function(phi,p,k){
  for (i in length(p):k+1){
    p[i]=phi[1]*p[i-1]+phi[2]*p[i-2]
  }
  return(p)
}
```
Aplicamos:
```{r}
coefs_acf_ej2<-c(1,-8/35,-461/700)
phi_ej2<-c(-0.4,-0.75)
coefs_acf_ej2<-coefs_acf(phi_ej2,coefs_acf_ej2,10)
ACF_2=data.frame(ACF=coefs_acf_ej2,lag=c(0:10))
ACF_2
```
c) Gráfica
```{r}
ggplot(ACF_2,aes(x=lag,y=ACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```
Continuación del inciso a).

Entonces:
```{r}
Var_Xt<-1/(1-sum(phi_ej2*(coefs_acf_ej2[2:3])))
Var_Xt
```
d) Sabemos que en un modelo AR(p), solo los primeros p coeficientes del PACF
son distintos de 0, es decir, en este caso los primeros 2, para lag=1 y 2.
Aplicamos la función escrita anteriormente:
```{r}
pacf_ej2=c()
for (i in 1:3){
  pacf_ej2[i]<-coefs_pacf(coefs_acf_ej2,i-1)
}
for (i in 4:11){
  pacf_ej2[i]<-0
}
PACF_2=data.frame(PACF=pacf_ej2,lag=c(0:10))
PACF_2
```
e) Graficamos

```{r}
ggplot(PACF_2,aes(x=lag,y=PACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```

f) Simulamos
```{r}
par(mfrow=c(1,1))
# Vamos a calcular el ACF de la muestra simulada
AR2=arima.sim(list(order=c(2,0,0), ar=c(-0.4,-0.75)),n=500)
plot(AR2, main="Simulación AR(2)", col="gold", lwd=2, las=1)
(ACFAR=ARMAacf(ar=c(-0.4,-0.75), ma=0,15))
plot(ACFAR,type="h", main="PACF de la muestra de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=1, col="violet")
acf(AR2)


# Vamos a calcular el PACF de la muestra simulada
(PACFAR=ARMAacf(ar=c(-0.4,-0.75), ma=0,15, pacf=T))
plot(PACFAR, type="h", lwd=3)
abline(h=0, lwd=1, col="violet")
pacf(AR2)
```

## Ejercicio 3

3.- Considere el siguiente proceso:

$$ X_{t} - 0.6X_{t-1} - 0.2X_{t-2} = Z_{t} + 0.4Z_{t-1} $$ 

(a) Escriba el proceso {$X_{t}$} en su forma de polinomio de retraso, donde {$Z_{t}$} es un ruido blanco.
(b) Clasifique el modelo como un $AR(p)$, $MA(q)$ o $ARMA(p,q)$ y defina el orden.
(c) Determina si el proceso es estacionario, invertible o causal.
(d) Obtenga la representación de un $AR(\infty)$ y $MA(\infty)$, respectivamente, si es que 

a)
\begin{align*}
Z_{t} &= X_{t} - 0.6 X_{t-1} - 0.2 X_{t-2} - 0.4 Z_{t-1}\\
      &= X_{t} - 0.6B_X{t} - 0.2B^{2}X_{t} - 0.4 BZ_{t}\\
\implies & (1 - 0.6B - 0.2B^{2})X_{t} = Z_{t} + 0.4BZ_{t}\\
      & (1 - 0.6B - 0.2B^{2})X_{t} = Z_{t} (1 +0.4B)\\
      & X_{t} = \frac{(1 + 0.4B)}{(1 - 0.6B - 0.2B^{2})}Z_{t} = \frac{\theta(B)}{\phi(B)}Z_{t}\\
      & \implies \phi(B)X_{t} = \theta(B)Z_{t} 
\end{align*} $_\blacksquare$

b) Sabemos que $X_{t}$ es un proceso autoregresivo de medias moviles $(ARMA(p,q)) si

$$ X_{t} = \phi_{1}X_{t-1} + \phi_{2}X_{t-2} + \dots + \phi_{p}X_{t-p} + Z_{t} + \theta_{1} Z_{t-1} + \theta_{2} Z_{t-2} + \dots + \theta_{q} Z_{t-q}$$
Equivalentemente

$$ X_{t} = \sum_{i=1}^{p} \phi_{i} Z_{t-1} + Z_{t} + \sum_{i=1}^{q} \theta_{i}Z_{t-1} $$
Ó 

$$ \phi(B)X_{t} = \theta(B)Z_{t}$$

Así, en nuestro caso

$$ X_{t} - 0.6X_{t-1} - 0.2X_{t-2} = Z_{t} + 0.4Z_{t-1}$$
es un proceso $ARMA(2,1)$, ya que lo podemos ver como:

\begin{align*}
& (1 - 0.6B - 0.2B^{2})X_{t} = (1 + 0.4B)Z_{t}\\
& \implies \phi(B)X_{t} = \theta(B)Z_{t}
\end{align*}

con $\phi(B)$ = $1 - 0.6B - 0.2B^{2}$ y $\theta(B)$ = 1 + 0.4B  $_\blacksquare$

c) Tenemos por definición de causalidad lo siguiente:

Un proceso $X_{t} ARMA(p,q)$ es causal si existe una sucesión $\psi_{j}$ y lo podemos escribir como:

$$ X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-1} $$

Esto quiere decir que el proceso $X_{t} puede escribirse como combinación lineal de los $Z_{t}$ anteriores y el actual.

Entonces para ver si  $X_{t}$ es causal, nos centramos en la parte autoregresiva pues la parte $MA$ es causal por contrucción.

Así que calculemos las raíces del polinomio $\phi(B)$:

Sea 
\begin{align*}
\phi(B) &= 1 - 0.6B - 0.2B^{2}\\
& \implies 1 - 0.6B - 0.2B^{2} = 0\\
& \implies -0.2B^{2} - 0.6B + 1 = 0\\
B_{1,2} &= \frac{-(-0.6) \pm \sqrt{(-0.6)^2 - 4 (-0.2)}}{2(-0.2)}\\
        &= \frac{(0.6) \pm \sqrt{1.16)}}{-0.4}
\end{align*}

Así obtenemos:

$$ B_{1} = \frac{-3}{2} + \frac{\sqrt{29}}{2}, B_{2} = \frac{-3}{2} - \frac{\sqrt{29}}{2}$$
Como |$\frac{-3}{2} + \frac{\sqrt{29}}{2}$| > 1 y |$\frac{-3}{2} - \frac{\sqrt{29}}{2}$| > 2, implica que el proceso es causal, ya que las raíces no caen dentro del círculo unitario.


Ahora para ver si es invertible, veremos la parte de promedios móviles:


$$ \theta(B) = 1 + 0.4B $$ 
\begin{align*}
& \implies 1 + 0.4B = 0\\
& \implies B = \frac{-1}{0.4}
\end{align*}

Sea |$-\frac{1}{0.4}$| > 1, esto implica que el proceso es invertible ya que B no cae cierto del círculo unitario.

$\therefore$ El proceso $X_{t}$ es invertible y causal $_\blacksquare$

d) 








