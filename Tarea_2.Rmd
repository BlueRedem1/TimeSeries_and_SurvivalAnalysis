---
title: "Tarea 2 Propiedades de los modelos ARMA"
author: "Cuéllar, Eduardo, García Jesús, Miranda Areli, Ramirez José, Saldaña Ricardo "
date: "10/26/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
1.- Considere el proceso $MA(2)$:

$$ X_t = Z_t - 0.4Z_{t-1} - 1.2Z_{t-2} $$
donde ${Z_{t}}$ es un ruido blanco Gaussiano.

(a) Calcule $\sigma^{2}_{X}$ suponiendo que $\sigma^{2}_{Z}$ = 1.
(b) Encuentre la expresión general para la función de autocorrelación $\rho_{k}$.
(c) Grafique $\rho_{k}$ (correlograma ACF), para $k= 0,1,2,...,10$.
(d) Encuentre la expresión general para la funció de autocorrelación parcial $\phi_{kk}$.
(e) Grafique $\phi_{kk}$ (correlograma PACF), para $k = 0,1,2,...,10$.
(f) En R simule el proceso ${X_{t}}$ para un tamaño de muestra $n$, grafique la serie de tiempo y los correlogramas ACF y PACF.
Compare los correlogramas simulados con los del proceso original.

```{r}
#Cargamos librerías
library(ggplot2);library(itsmr);library(forecast);library(TSA);library(lmtest)
library(timeSeries);library(timeSeries);library(astsa);
library(tseries);library(forecast);library(nortest)
```

Respueta: 

a) $Var$($X_{t}$) = $Var(Z_{t} - 0.4_{t-1}, - 1.2Z_{t-2})$ .... (1)
Como $Z_{k}$ $\perp$ $Z_{j}$ $\forall$ $k \ne j$

Podemos expresar a (1) de la siguiente manera:

$$ = Var(Z_{t}) + Var(-0.4 Z_{t-1}) + Var(-1.2Z_{t-2}) $$
$$ =  Var(Z_{t}) + (-0.4)^{2} Var(Z_{t-1}) + (-1.2)^{2} Var(Z_{t-2}) .... (2)$$
Como $Z_{t}$ son $v.a.i.i.d.$, con $\mathbb{E}[Z_{t}] = 0$ y $Var(Z_{t}) = 1$

$$ (2) = 1 + (.16)(1) + (1.44)(1) $$
$$ = 1 + .16 + 1.44 $$
$$ = 2.6  Cov(Z_{t} - 1.4Z_{t-1} - 1.2Z_{t-2}, Z_{t+k} - 0.4Z_{t-1+k} - Z_{t-2+k}) $$
b) Veamos la autocovarianza:

$$ \gamma(k) = Cov(X_{t}, X_{t+k})\\ $$
          $$ = Cov(Z_{t} - 0.4Z_{t-1} - 1.2Z_{t-2}, Z_{t+k} - 0.4Z_{t-1+k} - 1.2Z_{t-2+k})\\$$
          $$ = Cov(Z_{t}, Z{_t+k}) - 0.4 Cov(Z_{t}, Z_{t+k-1}) - 1.2 Cov(Z_{t}, Z_{t+k-2})\\$$
          $$ -0.4 Cov(Z_{t-1},Z_{t+k}) + .16Cov(Z_{t-1},Z_{t+k-1}) + .48Cov(Z_{t-1}, Z_{t+k-2})\\$$
          $$ -1.2Cov(Z_{t-2}, Z_{t+k}) + .48 Cov(Z_{t-2}, Z_{t+k_1}) + 1.44 Cov(Z_{t-2}, Z_{t+k-2})$$
 
 $$


Gráfica:
```{r}
acf_coefs_ej1=c(1,2/65,-6/13)
#llenemos de 0 los faltantes
for (i in (length(acf_coefs_ej1)+1):11){
  acf_coefs_ej1[i]=0
}
ACF_1<-data.frame('ACF'=acf_coefs_ej1,lag=0:10)
ACF_1
ggplot(ACF_1,aes(x=lag,y=ACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```


Automatizaremos la obtención de los coeficientes del PACF para el ejercicio 1:
```{r}
#x será el vector con los coeficientes de autocorrelación
coefs_pacf<-function(p,k){
  if(k==0){
    return(1)
  }
  if(length(p)<k+1){
    for(i in length(p):k){
      p[i+1]=0
    }
  }
  A<-matrix(nrow=k,ncol = k)
  for (j in 1:k){
    for (i in 1:k){
       A[i,j]=p[abs(i-j)+1]
    }
  }
  B<-A
  for (i in 1:k){
    B[i,k]=p[i+1]
  }
  return(det(B)/det(A))
}
```
¿Cómo funciona? Bien, en clase, en la página 27 de las notas, podemos observar
que $\phi_{kk}$, que es el coeficiente de autocorrelación parcial para un lag de $k$
se puede calcular usando Cramer. Observamos el patrón de que en la matriz que 
'va en el denominador', iba el coeficiente $\rho_{i}$ donde $i$ era el valor absoluto
de la diferencia entre el número de columna y renglón, por ello es que llenamos la
matriz como $A[i,j]=p[abs(i-j)+1]$. En la matriz 'numerador', únicamente es cambiar
el último renglón por los $\rho_j$ donde $j$ es el número de renglón, siendo ambas
matrices de dimensión $k*k$

Ahora solo aplicamos la fórmula:
```{r}
pacf_ej1=c()
for (i in 1:11){
  pacf_ej1[i]<-coefs_pacf(acf_coefs_ej1,i-1)
}
PACF_1=data.frame(PACF=pacf_ej1,lag=c(0:10))
PACF_1
```

Graficamos

```{r}
ggplot(PACF_1,aes(x=lag,y=PACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```

Simulamos 
```{r}
par(mfrow=c(1,1))
MA2=arima.sim(list(order=c(0,0,2), ma=c(-0.4,-1.2)),n=500)
plot(MA2, main="Simulación MA(2)", col="purple", ylab="", las=1, xlab="Tiempo")

##Vamos a calcular el ACF de manera teorica 
(ACFMA=ARMAacf( ma=c(-0.4,-1.2), ar=0,15))
plot(ACFMA[-1],type="h", main="ACF teorico de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=2, col="violet")
acf(MA2)
###?Que concluimos?
##Vamos a calcular el PACF de manera teorica 
(PACFMA=ARMAacf( ma=c(-0.4,-1.2), ar=0,15, pacf=T))
plot(PACFMA,type="h", main="PACF teorico de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=1, col="violet")
pacf(MA2)
```

##Ejercicio 2

Programamos la función recursiva para los coeficientes de correlación
```{r}
#phi es un vector con los coeficientes del modelo AR(p)
#p es un vector con los acf para lag=1 y 2
coefs_acf<-function(phi,p,k){
  for (i in length(p):k+1){
    p[i]=phi[1]*p[i-1]+phi[2]*p[i-2]
  }
  return(p)
}
```
Aplicamos:
```{r}
acf_coefs_ej2<-c(1,-8/35,-461/700)
phi_ej2<-c(-0.4,-0.75)
acf_coefs_ej2<-coefs_acf(phi_ej2,acf_coefs_ej2,10)
ACF_2=data.frame(ACF=acf_coefs_ej2,lag=c(0:10))
ACF_2
ggplot(ACF_2,aes(x=lag,y=ACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```

Entonces:
```{r}
Var_Xt<-1/(1-sum(phi_ej2*(acf_coefs_ej2[2:3])))
Var_Xt
```
Sabemos que en un modelo AR(p), solo los primeros p coeficientes del PACF
son distintos de 0, es decir, en este caso los primeros 2, para lag=1 y 2.
Aplicamos la función escrita anteriormente:
```{r}
pacf_ej2=c()
for (i in 1:3){
  pacf_ej2[i]<-coefs_pacf(acf_coefs_ej2,i-1)
}
for (i in 4:11){
  pacf_ej2[i]<-0
}
PACF_2=data.frame(PACF=pacf_ej2,lag=c(0:10))
PACF_2
```
Graficamos

```{r}
ggplot(PACF_2,aes(x=lag,y=PACF))+geom_point()+geom_abline(intercept = 0,slope=0,color='red')
```

Simulamos
```{r}
par(mfrow=c(1,1))
AR2=arima.sim(list(order=c(2,0,0), ar=c(-0.4,-0.75)),n=500)
plot(AR2, main="Simulación AR(2)", col="gold", lwd=2, las=1)
(ACFAR=ARMAacf(ar=c(-0.4,-0.75), ma=0,15))
plot(ACFAR,type="h", main="PACF teorico de un MA(2)", lwd=2, ylim=c(-1,1), las=1)
abline(h=0, lwd=1, col="violet")
acf(AR2)

#PACF

(PACFAR=ARMAacf(ar=c(-0.4,-0.75), ma=0,15, pacf=T))
plot(PACFAR, type="h", lwd=3)
abline(h=0, lwd=1, col="violet")
pacf(AR2)
```


